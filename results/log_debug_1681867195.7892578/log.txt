{'image_size': 32, 'in_channels': 256, 'out_channels': 256, 'model_channels': 32, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_scale_shift_norm': True, 'resblock_updown': True, 'cond_model': True}
cuda:  1
in this project, use device:  cuda
before loop:
num of input blocks: 1
num of input attns: 1
!!!!!!!!!!! time head: 16 !!!!!!!!!!!!!
!!! ABS_weight: 1.0
!!! MSE_weight: 0
!!! SSIM_weight: 0
contructing model...
ds_background:  3
num_hiddens:  256
num_residual_layers:  4
num_residual_hiddens:  128
suf_method:  conv
n_frames:  16
Loading Motion Encoder: Encoder_Motion...
DDPM: Running in eps-prediction mode
xt.shape: torch.Size([122, 3, 240, 320]) torch.float32
pre_img.shape: torch.Size([3, 256, 256])
nxt_img.shape: torch.Size([3, 256, 256])
video_x.shape:  torch.Size([1, 4, 32, 3, 256, 256])
show x.shape:  torch.Size([1, 4, 32, 3, 256, 256])
c.shape:  torch.Size([1, 4, 16, 3, 256, 256])
x.shape:  torch.Size([1, 4, 16, 3, 256, 256])
x dtype:  torch.float32
x_img.shape:  torch.Size([1, 16, 3, 256, 256]) torch.float32
x_bg.shape:  torch.Size([1, 16, 3, 256, 256]) torch.float32
x_id.shape:  torch.Size([1, 16, 3, 256, 256]) torch.float32
x_mo.shape:  torch.Size([1, 16, 3, 256, 256]) torch.float32
for bg encoder, _ds_m =  3 , xs.shape =  torch.Size([16, 3, 256, 256]) torch.float32
in bg _layer 0, xs.shape =  torch.Size([16, 256, 128, 128]) torch.float16
in bg _layer 1, xs.shape =  torch.Size([16, 256, 64, 64]) torch.float16
in bg _layer 2, xs.shape =  torch.Size([16, 256, 32, 32]) torch.float16
in bg encoder conv, xs.shape =  torch.Size([16, 256, 32, 32]) torch.float16
in bg encoder conv after rearrange, xs.shape =  torch.Size([1, 4096, 32, 32]) torch.float16
for id encoder: _ds_m =  4
for mo encoder: _ds_m =  5
feat_bg.shape:  torch.Size([1, 1, 256, 32, 32])
feat_id.shape:  torch.Size([1, 1, 256, 16, 16])
feat_mo.shape:  torch.Size([1, 16, 256, 8, 8])
for bg encoder, _ds_m =  3 , xs.shape =  torch.Size([16, 3, 256, 256]) torch.float32
in bg _layer 0, xs.shape =  torch.Size([16, 256, 128, 128]) torch.float16
in bg _layer 1, xs.shape =  torch.Size([16, 256, 64, 64]) torch.float16
in bg _layer 2, xs.shape =  torch.Size([16, 256, 32, 32]) torch.float16
in bg encoder conv, xs.shape =  torch.Size([16, 256, 32, 32]) torch.float16
in bg encoder conv after rearrange, xs.shape =  torch.Size([1, 4096, 32, 32]) torch.float16
for id encoder: _ds_m =  4
for mo encoder: _ds_m =  5
feat_bg.shape:  torch.Size([1, 1, 256, 32, 32])
feat_id.shape:  torch.Size([1, 1, 256, 16, 16])
feat_mo.shape:  torch.Size([1, 16, 256, 8, 8])
cbg_quantized.shape:  torch.Size([1, 32, 32]) torch.int64
cid_quantized.shape:  torch.Size([1, 16, 16]) torch.int64
cmo_quantized.shape:  torch.Size([16, 8, 8]) torch.int64
